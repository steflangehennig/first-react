{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aabfad6",
   "metadata": {},
   "source": [
    "# Evidence-Based Policy Scoring with Open LLMs (Zero-Shot)\n",
    "This notebook lets you score policy documents using an open-source LLM and a rubric-based prompt. It uses Mistral-7B via Hugging Face. Steps include:\n",
    "* Load Mistral-7B-Instruct via Hugging Face\n",
    "* Accept .txt policy files in a folder\n",
    "* Prompt the model using the evidence-based policy (EBP) rubric\n",
    "* Parse the response and output a .csv with scores and justifications for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install -q unstructured\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf27ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True)\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff720",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_prompt = \"\"\"You are a policy analyst evaluating how evidence-based a policy document is.\n",
    "Use the rubric below to assess the document on a 0‚Äì3 scale for each dimension.\n",
    "Provide both a score and a short justification for each.\n",
    "\n",
    "### Rubric\n",
    "1. Use of Empirical Research\n",
    "- 0: No references to empirical evidence or data\n",
    "- 1: Vague or anecdotal references (e.g., ‚Äústudies show‚Äù)\n",
    "- 2: Clear empirical support, but limited sourcing\n",
    "- 3: Multiple, clearly cited, high-quality sources (e.g., peer-reviewed, systematic reviews)\n",
    "\n",
    "2. Formal Evidence-Gathering Process\n",
    "- 0: No structured data gathering\n",
    "- 1: Informal or anecdotal input\n",
    "- 2: Basic assessments (e.g., internal reports, cost estimates)\n",
    "- 3: Formal tools (e.g., RCTs, modeling, pilot programs)\n",
    "\n",
    "3. Transparency and Accessibility\n",
    "- 0: No documentation or rationale\n",
    "- 1: Minimal or internal-only documentation\n",
    "- 2: Public access with basic explanation\n",
    "- 3: Fully open access, replicable, with detailed methods\n",
    "\n",
    "4. Expert and Stakeholder Input\n",
    "- 0: No input from external experts or stakeholders\n",
    "- 1: Informal or internal-only consultation\n",
    "- 2: Formal expert or stakeholder involvement\n",
    "- 3: Broad, interdisciplinary consultation, including marginalized groups\n",
    "\n",
    "5. Evaluation and Iteration\n",
    "- 0: No evaluation mechanism\n",
    "- 1: Evaluation mentioned but vague\n",
    "- 2: Evaluation planned or metrics included\n",
    "- 3: Evaluation built-in with feedback loops\n",
    "\n",
    "### Document:\n",
    "{document_text}\n",
    "\n",
    "### Task:\n",
    "Provide a JSON-formatted output like this:\n",
    "{\n",
    "  \"Use of Empirical Research\": {\"score\": 2, \"justification\": \"...\"},\n",
    "  \"Formal Evidence-Gathering Process\": {\"score\": 1, \"justification\": \"...\"},\n",
    "  \"Transparency and Accessibility\": {\"score\": 3, \"justification\": \"...\"},\n",
    "  \"Expert and Stakeholder Input\": {\"score\": 1, \"justification\": \"...\"},\n",
    "  \"Evaluation and Iteration\": {\"score\": 2, \"justification\": \"...\"}\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb096ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/content/policies\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Upload your .txt policy documents into: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74697dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_text = f.read()\n",
    "\n",
    "    full_prompt = rubric_prompt.replace(\"{document_text}\", doc_text[:4000])  # truncate if needed\n",
    "    response = llm(full_prompt)[0][\"generated_text\"]\n",
    "\n",
    "    try:\n",
    "        json_start = response.find(\"{\")\n",
    "        json_end = response.rfind(\"}\") + 1\n",
    "        parsed = json.loads(response[json_start:json_end])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse output for {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    flat = {\"filename\": filename}\n",
    "    for k, v in parsed.items():\n",
    "        flat[f\"{k} Score\"] = v.get(\"score\")\n",
    "        flat[f\"{k} Justification\"] = v.get(\"justification\")\n",
    "    results.append(flat)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"/content/evidence_scores.csv\", index=False)\n",
    "print(\"‚úÖ Scoring complete! Download your CSV from /content/evidence_scores.csv\")\n",
    "df.head()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
