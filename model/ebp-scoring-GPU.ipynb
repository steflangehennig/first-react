{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca1d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi  # Confirm GPU is active "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aabfad6",
   "metadata": {},
   "source": [
    "# Evidence-Based Policy Scoring with Open LLMs (Zero-Shot)\n",
    "\n",
    "This notebook lets you score policy documents using an open-source LLM and a rubric-based prompt. It uses Mistral-7B via Hugging Face. Steps include:\n",
    "* Load Mistral-7B-Instruct via Hugging Face\n",
    "* Accept .txt policy files in a folder\n",
    "* Prompt the model using the evidence-based policy (EBP) rubric\n",
    "* Parse the response and output a .csv with scores and justifications for them\n",
    "\n",
    "> Go to Runtime > Change runtime type > GPU in Colab if not doing it locally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate bitsandbytes sentencepiece\n",
    "!pip install -q unstructured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853b88",
   "metadata": {},
   "source": [
    "### Loading in models\n",
    "Note:  Mistral is a huge model and will take a long time to cache initially. It will also take a long time to process docs through it. A smaller model, like Llama-3b, is easier to use locally via CPU. Use a GPU (e.g., Colab) to run Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf27ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dff720",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_prompt = \"\"\"You are a policy analyst evaluating how evidence-based a policy document is.\n",
    "Use the rubric below to assess the document on a 0‚Äì3 scale for each dimension.\n",
    "Provide both a score and a short justification for each.\n",
    "\n",
    "### Rubric\n",
    "1. Use of Empirical Research\n",
    "- 0: No references to empirical evidence or data\n",
    "- 1: Vague or anecdotal references (e.g., ‚Äústudies show‚Äù)\n",
    "- 2: Clear empirical support, but limited sourcing\n",
    "- 3: Multiple, clearly cited, high-quality sources (e.g., peer-reviewed, systematic reviews)\n",
    "\n",
    "2. Formal Evidence-Gathering Process\n",
    "- 0: No structured data gathering\n",
    "- 1: Informal or anecdotal input\n",
    "- 2: Basic assessments (e.g., internal reports, cost estimates)\n",
    "- 3: Formal tools (e.g., RCTs, modeling, pilot programs)\n",
    "\n",
    "3. Transparency and Accessibility\n",
    "- 0: No documentation or rationale\n",
    "- 1: Minimal or internal-only documentation\n",
    "- 2: Public access with basic explanation\n",
    "- 3: Fully open access, replicable, with detailed methods\n",
    "\n",
    "4. Expert and Stakeholder Input\n",
    "- 0: No input from external experts or stakeholders\n",
    "- 1: Informal or internal-only consultation\n",
    "- 2: Formal expert or stakeholder involvement\n",
    "- 3: Broad, interdisciplinary consultation, including marginalized groups\n",
    "\n",
    "5. Evaluation and Iteration\n",
    "- 0: No evaluation mechanism\n",
    "- 1: Evaluation mentioned but vague\n",
    "- 2: Evaluation planned or metrics included\n",
    "- 3: Evaluation built-in with feedback loops\n",
    "\n",
    "### Document:\n",
    "{document_text}\n",
    "\n",
    "### Task:\n",
    "Provide a JSON-formatted output like this:\n",
    "{\n",
    "  \"Use of Empirical Research\": {\"score\": 2, \"justification\": \"...\"},\n",
    "  \"Formal Evidence-Gathering Process\": {\"score\": 1, \"justification\": \"...\"},\n",
    "  \"Transparency and Accessibility\": {\"score\": 3, \"justification\": \"...\"},\n",
    "  \"Expert and Stakeholder Input\": {\"score\": 1, \"justification\": \"...\"},\n",
    "  \"Evaluation and Iteration\": {\"score\": 2, \"justification\": \"...\"}\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb096ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"/model/txt\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "print(f\"Upload your .txt policy documents into: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "txt_folder = Path(\"txt\")  # or wherever your .txt files are\n",
    "results = []\n",
    "\n",
    "for filename in os.listdir(txt_folder):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    print(f\"üîç Processing {filename}\")\n",
    "    with open(txt_folder / filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc_text = f.read()\n",
    "\n",
    "    full_prompt = rubric_prompt.replace(\"{document_text}\", doc_text[:4000])\n",
    "    response = llm(full_prompt)[0][\"generated_text\"]\n",
    "    print(\"üß† Raw response:\\n\", response[:500], \"\\n---\")\n",
    "\n",
    "    try:\n",
    "        json_start = response.find(\"{\")\n",
    "        json_end = response.rfind(\"}\") + 1\n",
    "        parsed = json.loads(response[json_start:json_end])\n",
    "        print(\"‚úÖ Parsed JSON\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to parse output for {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    flat = {\"filename\": filename}\n",
    "    for k, v in parsed.items():\n",
    "        flat[f\"{k} Score\"] = v.get(\"score\")\n",
    "        flat[f\"{k} Justification\"] = v.get(\"justification\")\n",
    "    results.append(flat)\n",
    "\n",
    "# Save if any results made it\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"evidence_scores.csv\", index=False)\n",
    "    print(\"‚úÖ Scoring complete! CSV saved to model/evidence_scores.csv\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results were successfully parsed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
